{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 crossarms, 1 polo, 9 wires, 30.0ms\n",
      "Speed: 3.0ms preprocess, 30.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 crossarms, 1 polo, 12 wires, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Image 1: 12 objects detected\n",
      "Image 2: 14 objects detected\n",
      "Added objects: 2\n",
      "Removed objects: 0\n",
      "\n",
      "Class distribution in Image 1:\n",
      "Class 2: 8\n",
      "Class 0: 3\n",
      "Class 1: 1\n",
      "\n",
      "Class distribution in Image 2:\n",
      "Class 2: 10\n",
      "Class 0: 3\n",
      "Class 1: 1\n"
     ]
    }
   ],
   "source": [
    "# 2D CNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 2D CNN 모델 정의\n",
    "class Simple2DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple2DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 160 * 160, 512)  # 640x640 크기에 맞게 조정\n",
    "        self.fc2 = nn.Linear(512, 2)  # 예시: 클래스가 2개 (변화 감지, 비변화)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 64 * 160 * 160)  # Flatten\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def extract_objects(results, confidence_threshold):\n",
    "    objects = []\n",
    "    for r in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = r\n",
    "        if score > confidence_threshold:\n",
    "            objects.append({\n",
    "                'bbox': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'score': score,\n",
    "                'class_id': int(class_id)\n",
    "            })\n",
    "    return objects\n",
    "\n",
    "def compare_objects(objects1, objects2, iou_threshold):\n",
    "    added = []\n",
    "    removed = []\n",
    "    \n",
    "    matched1 = set()\n",
    "    matched2 = set()\n",
    "    \n",
    "    for i, obj1 in enumerate(objects1):\n",
    "        for j, obj2 in enumerate(objects2):\n",
    "            if i in matched1 or j in matched2:\n",
    "                continue\n",
    "            \n",
    "            iou = calculate_iou(obj1['bbox'], obj2['bbox'])\n",
    "            if iou > iou_threshold:\n",
    "                matched1.add(i)\n",
    "                matched2.add(j)\n",
    "                break\n",
    "        \n",
    "        if i not in matched1:\n",
    "            removed.append(obj1)\n",
    "    \n",
    "    for j, obj2 in enumerate(objects2):\n",
    "        if j not in matched2:\n",
    "            added.append(obj2)\n",
    "    \n",
    "    return added, removed\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    iou = intersection / float(area1 + area2 - intersection)\n",
    "    return iou\n",
    "    pass    \n",
    "\n",
    "def visualize_changes(image1, image2, added, removed):\n",
    "    result_image = np.hstack((image1, image2))\n",
    "    height, width = image1.shape[:2]\n",
    "    \n",
    "    for obj in added:\n",
    "        x1, y1, x2, y2 = obj['bbox']\n",
    "        cv2.rectangle(result_image, (x1 + width, y1), (x2 + width, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(result_image, 'Added', (x1 + width, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    for obj in removed:\n",
    "        x1, y1, x2, y2 = obj['bbox']\n",
    "        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        cv2.putText(result_image, 'Removed', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow('Changes Detected', result_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    pass\n",
    "\n",
    "def analyze_results(objects1, objects2, added, removed):\n",
    "    print(f\"Image 1: {len(objects1)} objects detected\")\n",
    "    print(f\"Image 2: {len(objects2)} objects detected\")\n",
    "    print(f\"Added objects: {len(added)}\")\n",
    "    print(f\"Removed objects: {len(removed)}\")\n",
    "    \n",
    "    class_counts1 = {}\n",
    "    class_counts2 = {}\n",
    "    \n",
    "    for obj in objects1:\n",
    "        class_counts1[obj['class_id']] = class_counts1.get(obj['class_id'], 0) + 1\n",
    "    \n",
    "    for obj in objects2:\n",
    "        class_counts2[obj['class_id']] = class_counts2.get(obj['class_id'], 0) + 1\n",
    "    \n",
    "    print(\"\\nClass distribution in Image 1:\")\n",
    "    for class_id, count in class_counts1.items():\n",
    "        print(f\"Class {class_id}: {count}\")\n",
    "    \n",
    "    print(\"\\nClass distribution in Image 2:\")\n",
    "    for class_id, count in class_counts2.items():\n",
    "        print(f\"Class {class_id}: {count}\")\n",
    "\n",
    "\n",
    "# 변화 감지 함수\n",
    "def detect_changes_2d(image1_path, image2_path, model_path, cnn_model, confidence_threshold=0.5, iou_threshold=0.5):\n",
    "    # YOLO 모델 로드\n",
    "    model = YOLO(model_path)\n",
    "\n",
    "    # 이미지 로드\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "\n",
    "    # YOLO를 사용하여 객체 탐지\n",
    "    results1 = model(image1)[0]\n",
    "    results2 = model(image2)[0]\n",
    "\n",
    "    # 객체 추출\n",
    "    objects1 = extract_objects(results1, confidence_threshold)\n",
    "    objects2 = extract_objects(results2, confidence_threshold)\n",
    "\n",
    "    # 객체 비교 (기존 방식 유지)\n",
    "    added, removed = compare_objects(objects1, objects2, iou_threshold)\n",
    "\n",
    "    # 이미지를 2D CNN에 입력\n",
    "    image1_2d = prepare_image_for_2d_cnn(image1)\n",
    "    image2_2d = prepare_image_for_2d_cnn(image2)\n",
    "\n",
    "    # 2D CNN을 통한 특징 추출 및 변화 감지\n",
    "    features1 = cnn_model(image1_2d)\n",
    "    features2 = cnn_model(image2_2d)\n",
    "    \n",
    "    difference = compute_difference_2d(features1, features2)\n",
    "\n",
    "    # 결과 시각화\n",
    "    visualize_changes(image1, image2, added, removed)\n",
    "\n",
    "    # 결과 분석\n",
    "    analyze_results(objects1, objects2, added, removed)\n",
    "\n",
    "# 2D CNN에 입력할 이미지를 준비하는 함수\n",
    "def prepare_image_for_2d_cnn(image):\n",
    "    # 이미지를 2D CNN에 입력할 수 있도록 변환\n",
    "    image = cv2.resize(image, (640, 640))\n",
    "    image_2d = np.transpose(image, (2, 0, 1))  # (H, W, C) -> (C, H, W)\n",
    "    return torch.Tensor(image_2d).unsqueeze(0)  # (B, C, H, W)\n",
    "\n",
    "# 2D CNN을 통한 특징 비교 함수\n",
    "def compute_difference_2d(features1, features2):\n",
    "    # 간단한 유클리드 거리 계산\n",
    "    return torch.norm(features1 - features2, p=2).item()\n",
    "\n",
    "# 나머지 기존 함수들 유지 (extract_objects, compare_objects, calculate_iou, visualize_changes, analyze_results)\n",
    "\n",
    "# CNN 모델 초기화\n",
    "cnn_model = Simple2DCNN()\n",
    "\n",
    "# 사용 예시\n",
    "image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 1.jpg'\n",
    "image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 2.jpg'\n",
    "model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "\n",
    "detect_changes_2d(image1_path, image2_path, model_path, cnn_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1: 11 objects detected\n",
      "Image 2: 15 objects detected\n",
      "Added objects: 4\n",
      "Removed objects: 0\n",
      "\n",
      "Class distribution in Image 1:\n",
      "Class 0: 2\n",
      "Class 2: 8\n",
      "Class 1: 1\n",
      "\n",
      "Class distribution in Image 2:\n",
      "Class 0: 2\n",
      "Class 2: 12\n",
      "Class 1: 1\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Simple2DCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Simple2DCNN, self).__init__()\n",
    "        # 더 깊은 CNN 구조로 변경\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(256 * 80 * 80, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.pool(torch.relu(self.batch_norm3(self.conv3(x))))\n",
    "        x = x.view(-1, 256 * 80 * 80)\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def extract_objects(results, confidence_threshold):\n",
    "    objects = []\n",
    "    # 신뢰도 점수에 따른 정렬 추가\n",
    "    boxes = sorted(results.boxes.data.tolist(), key=lambda x: x[4], reverse=True)\n",
    "    \n",
    "    for r in boxes:\n",
    "        x1, y1, x2, y2, score, class_id = r\n",
    "        if score > confidence_threshold:\n",
    "            # 바운딩 박스 정규화 추가\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            if w > 0 and h > 0:  # 유효한 바운딩 박스만 추가\n",
    "                objects.append({\n",
    "                    'bbox': [x1, y1, x2, y2],\n",
    "                    'score': score,\n",
    "                    'class_id': int(class_id),\n",
    "                    'area': w * h  # 면적 정보 추가\n",
    "                })\n",
    "    return objects\n",
    "\n",
    "def compare_objects(objects1, objects2, iou_threshold):\n",
    "    added = []\n",
    "    removed = []\n",
    "    matched1 = set()\n",
    "    matched2 = set()\n",
    "    \n",
    "    # 면적 기반 정렬 추가\n",
    "    objects1 = sorted(objects1, key=lambda x: x['area'], reverse=True)\n",
    "    objects2 = sorted(objects2, key=lambda x: x['area'], reverse=True)\n",
    "    \n",
    "    for i, obj1 in enumerate(objects1):\n",
    "        best_iou = iou_threshold\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, obj2 in enumerate(objects2):\n",
    "            if j in matched2:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(obj1['bbox'], obj2['bbox'])\n",
    "            # 클래스 ID가 같은 경우에만 매칭\n",
    "            if iou > best_iou and obj1['class_id'] == obj2['class_id']:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            matched1.add(i)\n",
    "            matched2.add(best_match)\n",
    "        else:\n",
    "            removed.append(obj1)\n",
    "    \n",
    "    for j, obj2 in enumerate(objects2):\n",
    "        if j not in matched2:\n",
    "            added.append(obj2)\n",
    "    \n",
    "    return added, removed\n",
    "\n",
    "def detect_changes_2d(image1_path, image2_path, model_path, cnn_model, \n",
    "                     confidence_threshold=0.45,  # 신뢰도 임계값 조정\n",
    "                     iou_threshold=0.4):        # IOU 임계값 조정\n",
    "    # YOLO 모델 로드 및 설정\n",
    "    model = YOLO(model_path)\n",
    "    model.conf = confidence_threshold  # 모델 신뢰도 임계값 설정\n",
    "    model.iou = iou_threshold         # 모델 IOU 임계값 설정\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 정규화\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # YOLO 예측\n",
    "    results1 = model(image1, verbose=False)[0]  # verbose=False로 불필요한 출력 제거\n",
    "    results2 = model(image2, verbose=False)[0]\n",
    "    \n",
    "    # 객체 추출 및 비교\n",
    "    objects1 = extract_objects(results1, confidence_threshold)\n",
    "    objects2 = extract_objects(results2, confidence_threshold)\n",
    "    \n",
    "    # 변화 감지\n",
    "    added, removed = compare_objects(objects1, objects2, iou_threshold)\n",
    "    \n",
    "    # CNN 특징 추출\n",
    "    with torch.no_grad():  # 추론 모드로 변경\n",
    "        image1_2d = prepare_image_for_2d_cnn(image1)\n",
    "        image2_2d = prepare_image_for_2d_cnn(image2)\n",
    "        features1 = cnn_model(image1_2d)\n",
    "        features2 = cnn_model(image2_2d)\n",
    "    \n",
    "    # 결과 시각화 및 분석\n",
    "    visualize_changes(image1, image2, added, removed)\n",
    "    analyze_results(objects1, objects2, added, removed)\n",
    "    \n",
    "    return added, removed\n",
    "\n",
    "def prepare_image_for_2d_cnn(image):\n",
    "    # 이미지 정규화 추가\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    image = torch.Tensor(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 7.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 8.jpg'\n",
    "    model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    # CNN 모델 초기화 및 평가 모드로 설정\n",
    "    cnn_model = Simple2DCNN()\n",
    "    cnn_model.eval()\n",
    "    \n",
    "    # 변화 감지 실행\n",
    "    detect_changes_2d(image1_path, image2_path, model_path, cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 crossarms, 1 polo, 15 wires, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 crossarms, 1 polo, 19 wires, 12.0ms\n",
      "Speed: 3.0ms preprocess, 12.0ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Change probability: 55.55%\n",
      "Change detected: True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Simple2DCNN(nn.Module):\n",
    "    def __init__(self, input_channels=3):\n",
    "        super(Simple2DCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(128 * 160 * 160, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # 변화 있음/없음 분류\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.pool(torch.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = x.view(-1, 128 * 160 * 160)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_detection_map(image_shape, detections, confidence_threshold=0.25):\n",
    "    \"\"\"YOLO 검출 결과를 히트맵으로 변환\"\"\"\n",
    "    detection_map = np.zeros(image_shape[:2], dtype=np.float32)\n",
    "    \n",
    "    for det in detections.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = det\n",
    "        if conf > confidence_threshold:\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            detection_map[y1:y2, x1:x2] = conf\n",
    "    \n",
    "    return detection_map\n",
    "\n",
    "def detect_changes(image1_path, image2_path, yolo_model_path, cnn_model=None):\n",
    "    # YOLO 모델 로드\n",
    "    yolo = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 이미지 로드 및 크기 조정\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_s    ize)\n",
    "    \n",
    "    # YOLO로 객체 검출\n",
    "    results1 = yolo(image1)[0]\n",
    "    results2 = yolo(image2)[0]\n",
    "    \n",
    "    # 검출 결과를 히트맵으로 변환\n",
    "    detection_map1 = create_detection_map(image1.shape, results1)\n",
    "    detection_map2 = create_detection_map(image2.shape, results2)\n",
    "    \n",
    "    # 검출 맵을 3채널로 확장 (CNN 입력용)\n",
    "    detection_maps = np.stack([\n",
    "        detection_map1,\n",
    "        detection_map2,\n",
    "        np.abs(detection_map1 - detection_map2)\n",
    "    ], axis=2)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    visualize_results(image1, image2, detection_map1, detection_map2, results1, results2)\n",
    "    \n",
    "    # CNN 모델이 제공된 경우 변화 분류 수행\n",
    "    if cnn_model is not None:\n",
    "        with torch.no_grad():\n",
    "            # 입력 데이터 준비\n",
    "            input_tensor = torch.FloatTensor(detection_maps).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            # 변화 예측\n",
    "            prediction = cnn_model(input_tensor)\n",
    "            probability = torch.softmax(prediction, dim=1)\n",
    "            change_prob = probability[0][1].item()\n",
    "            \n",
    "            print(f\"Change probability: {change_prob:.2%}\")\n",
    "            return change_prob > 0.5\n",
    "    \n",
    "    return None\n",
    "\n",
    "def visualize_results(image1, image2, map1, map2, yolo_results1, yolo_results2):\n",
    "    # YOLO 검출 결과 시각화\n",
    "    det_image1 = image1.copy()\n",
    "    det_image2 = image2.copy()\n",
    "    \n",
    "    # YOLO 바운딩 박스 그리기\n",
    "    for result, img in [(yolo_results1, det_image1), (yolo_results2, det_image2)]:\n",
    "        for box in result.boxes.data.tolist():\n",
    "            x1, y1, x2, y2, conf, cls = box\n",
    "            if conf > 0.25:  # 신뢰도 임계값\n",
    "                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(img, f'{conf:.2f}', (x1, y1-10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    # 히트맵 시각화\n",
    "    map1_colored = cv2.applyColorMap((map1 * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    map2_colored = cv2.applyColorMap((map2 * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "    \n",
    "    # 결과 합치기\n",
    "    top_row = np.hstack((det_image1, det_image2))\n",
    "    bottom_row = np.hstack((map1_colored, map2_colored))\n",
    "    combined = np.vstack((top_row, bottom_row))\n",
    "    \n",
    "    # 창 크기 조정\n",
    "    scale = 0.7\n",
    "    display_size = (int(combined.shape[1] * scale), int(combined.shape[0] * scale))\n",
    "    combined = cv2.resize(combined, display_size)\n",
    "    \n",
    "    cv2.imshow('Detection Results', combined)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 5.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 6.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    # 2D CNN 모델 초기화 (선택적)\n",
    "    cnn_model = Simple2DCNN(input_channels=3)\n",
    "    \n",
    "    # 변화 감지 실행\n",
    "    result = detect_changes(image1_path, image2_path, yolo_model_path, cnn_model)\n",
    "    \n",
    "    if result is not None:\n",
    "        print(f\"Change detected: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crossarm, 1 polo, 13 wires, 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 86.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crossarm, 1 polo, 15 wires, 11.4ms\n",
      "Speed: 2.0ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Total objects in first image: 15\n",
      "Total objects in second image: 17\n",
      "Disappeared objects: 0\n",
      "Appeared objects: 2\n",
      "\n",
      "Appeared objects details:\n",
      "Class: 2, Confidence: 0.94\n",
      "Class: 2, Confidence: 0.93\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"두 바운딩 박스 간의 IoU 계산\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    iou = intersection / float(area1 + area2 - intersection + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25, iou_threshold=0.5):\n",
    "    # YOLO 모델 로드\n",
    "    model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 이미지 로드\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # YOLO로 객체 검출\n",
    "    results1 = model(image1)[0]\n",
    "    results2 = model(image2)[0]\n",
    "    \n",
    "    # 검출 결과 추출\n",
    "    boxes1 = []\n",
    "    boxes2 = []\n",
    "    \n",
    "    for r in results1.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes1.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    for r in results2.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes2.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    # 객체 매칭 및 변화 감지\n",
    "    disappeared = []  # 사라진 객체\n",
    "    appeared = []    # 새로 나타난 객체\n",
    "    \n",
    "    # 같은 객체 매칭\n",
    "    for i, obj1 in enumerate(boxes1):\n",
    "        best_iou = iou_threshold\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, obj2 in enumerate(boxes2):\n",
    "            if obj2['matched'] or obj1['class'] != obj2['class']:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(obj1['box'], obj2['box'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            boxes1[i]['matched'] = True\n",
    "            boxes2[best_match]['matched'] = True\n",
    "        else:\n",
    "            disappeared.append(obj1)\n",
    "    \n",
    "    # 새로 나타난 객체 찾기\n",
    "    for obj2 in boxes2:\n",
    "        if not obj2['matched']:\n",
    "            appeared.append(obj2)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    def create_visualization(image1, image2, disappeared, appeared):\n",
    "        # 원본 이미지 복사\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 사라진 객체 표시 (빨간색)\n",
    "        for obj in disappeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img1, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(vis_img1, f'Disappeared (conf: {obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체 표시 (초록색)\n",
    "        for obj in appeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(vis_img2, f'Appeared (conf: {obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 히트맵 생성\n",
    "        heatmap1 = np.zeros(image1.shape[:2], dtype=np.float32)\n",
    "        heatmap2 = np.zeros(image2.shape[:2], dtype=np.float32)\n",
    "        \n",
    "        # 사라진 객체 히트맵\n",
    "        for obj in disappeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            heatmap1[y1:y2, x1:x2] = obj['conf']\n",
    "        \n",
    "        # 새로 나타난 객체 히트맵\n",
    "        for obj in appeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            heatmap2[y1:y2, x1:x2] = obj['conf']\n",
    "        \n",
    "        # 히트맵 컬러 변환\n",
    "        heatmap1_colored = cv2.applyColorMap((heatmap1 * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        heatmap2_colored = cv2.applyColorMap((heatmap2 * 255).astype(np.uint8), cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, heatmap1_colored, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, heatmap2_colored, alpha, 0)\n",
    "        \n",
    "        # 최종 결과 이미지 생성\n",
    "        result = np.hstack((overlay1, overlay2))\n",
    "        \n",
    "        return result, heatmap1_colored, heatmap2_colored\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image, hmap1, hmap2 = create_visualization(image1, image2, disappeared, appeared)\n",
    "    \n",
    "    # 결과 출력\n",
    "    cv2.imshow('Object Changes', result_image)\n",
    "    \n",
    "    # 분석 결과 출력\n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Total objects in first image: {len(boxes1)}\")\n",
    "    print(f\"Total objects in second image: {len(boxes2)}\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "# 실행 코드\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 3.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 4.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    disappeared, appeared = detect_object_changes(\n",
    "        image1_path, \n",
    "        image2_path, \n",
    "        yolo_model_path,\n",
    "        conf_threshold=0.25,  # 신뢰도 임계값\n",
    "        iou_threshold=0.5     # IoU 임계값\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 crossarms, 1 polo, 10 wires, 16.7ms\n",
      "Speed: 3.0ms preprocess, 16.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 crossarms, 1 polo, 13 wires, 17.3ms\n",
      "Speed: 2.0ms preprocess, 17.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Total objects in first image: 13\n",
      "Total objects in second image: 16\n",
      "Disappeared objects: 0\n",
      "Appeared objects: 3\n",
      "Modified objects: 13\n",
      "\n",
      "Appeared objects details:\n",
      "Class: 2, Confidence: 0.65\n",
      "Class: 2, Confidence: 0.57\n",
      "Class: 2, Confidence: 0.40\n",
      "\n",
      "Modified objects details:\n",
      "Class: 0, Confidence: 0.86\n",
      "Class: 0, Confidence: 0.86\n",
      "Class: 2, Confidence: 0.83\n",
      "Class: 2, Confidence: 0.79\n",
      "Class: 2, Confidence: 0.79\n",
      "Class: 2, Confidence: 0.76\n",
      "Class: 2, Confidence: 0.76\n",
      "Class: 2, Confidence: 0.66\n",
      "Class: 2, Confidence: 0.53\n",
      "Class: 2, Confidence: 0.52\n",
      "Class: 1, Confidence: 0.51\n",
      "Class: 2, Confidence: 0.40\n",
      "Class: 2, Confidence: 0.32\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First 3D Conv layer\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(2, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            \n",
    "            # Second 3D Conv layer\n",
    "            nn.Conv3d(32, 16, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(16),\n",
    "            \n",
    "            # Final layer for change detection\n",
    "            nn.Conv3d(16, 1, kernel_size=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)  # temporal dimension 제거\n",
    "        return x\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"두 바운딩 박스 간의 IoU 계산\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    iou = intersection / float(area1 + area2 - intersection + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25, iou_threshold=0.5):\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 3D CNN 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    temporal_model = Conv3DNet().to(device)\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # YOLO로 객체 검출\n",
    "    results1 = yolo_model(image1)[0]\n",
    "    results2 = yolo_model(image2)[0]\n",
    "    \n",
    "    # 3D CNN을 위한 입력 준비\n",
    "    imgs_tensor = torch.from_numpy(np.stack([image1, image2]))\n",
    "    imgs_tensor = imgs_tensor.permute(3, 0, 1, 2).unsqueeze(0).float() / 255.0\n",
    "    imgs_tensor = imgs_tensor.to(device)\n",
    "    \n",
    "    # 3D CNN으로 변화 감지\n",
    "    with torch.no_grad():\n",
    "        change_score = temporal_model(imgs_tensor)\n",
    "        change_score = change_score.squeeze()\n",
    "    \n",
    "    # 검출 결과 추출\n",
    "    boxes1 = []\n",
    "    boxes2 = []\n",
    "    \n",
    "    for r in results1.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes1.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    for r in results2.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes2.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    # 객체 매칭 및 변화 감지\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    modified = []\n",
    "    \n",
    "    for i, obj1 in enumerate(boxes1):\n",
    "        best_iou = iou_threshold\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, obj2 in enumerate(boxes2):\n",
    "            if obj2['matched'] or obj1['class'] != obj2['class']:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(obj1['box'], obj2['box'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            boxes1[i]['matched'] = True\n",
    "            boxes2[best_match]['matched'] = True\n",
    "            \n",
    "            # 변화 스코어를 확인하여 수정된 객체 감지\n",
    "            x1, y1, x2, y2 = obj1['box']\n",
    "            change_region = change_score[y1:y2, x1:x2].mean().item()\n",
    "            if change_region > 0.3:\n",
    "                modified.append(obj1)\n",
    "        else:\n",
    "            disappeared.append(obj1)\n",
    "    \n",
    "    # 새로 나타난 객체 찾기\n",
    "    for obj2 in boxes2:\n",
    "        if not obj2['matched']:\n",
    "            appeared.append(obj2)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    def create_visualization(image1, image2, disappeared, appeared, modified, change_score):\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 사라진 객체 표시 (빨간색)\n",
    "        for obj in disappeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img1, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(vis_img1, f'Disappeared ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체 표시 (초록색)\n",
    "        for obj in appeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(vis_img2, f'Appeared ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 수정된 객체 표시 (파란색)\n",
    "        for obj in modified:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img1, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.rectangle(vis_img2, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            cv2.putText(vis_img1, f'Modified ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "            cv2.putText(vis_img2, f'Modified ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "        # 변화 히트맵 생성\n",
    "        change_heatmap = change_score.cpu().numpy()\n",
    "        change_heatmap = (change_heatmap * 255).astype(np.uint8)\n",
    "        change_heatmap = cv2.applyColorMap(change_heatmap, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        \n",
    "        result = np.hstack((overlay1, overlay2))\n",
    "        return result\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image = create_visualization(image1, image2, disappeared, appeared, modified, change_score)\n",
    "    \n",
    "    # 결과 출력\n",
    "    cv2.imshow('Object Changes (3D CNN)', result_image)\n",
    "    \n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Total objects in first image: {len(boxes1)}\")\n",
    "    print(f\"Total objects in second image: {len(boxes2)}\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    print(f\"Modified objects: {len(modified)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "            \n",
    "    if modified:\n",
    "        print(\"\\nModified objects details:\")\n",
    "        for obj in modified:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared, modified\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 7.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 8.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    disappeared, appeared, modified = detect_object_changes(\n",
    "        image1_path, \n",
    "        image2_path, \n",
    "        yolo_model_path,\n",
    "        conf_threshold=0.25,\n",
    "        iou_threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 crossarms, 1 polo, 10 wires, 13.0ms\n",
      "Speed: 2.0ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 crossarms, 1 polo, 13 wires, 14.0ms\n",
      "Speed: 2.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Total objects in first image: 13\n",
      "Total objects in second image: 16\n",
      "Disappeared objects: 0\n",
      "Appeared objects: 3\n",
      "\n",
      "Appeared objects details:\n",
      "Class: 2, Confidence: 0.65\n",
      "Class: 2, Confidence: 0.57\n",
      "Class: 2, Confidence: 0.40\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # First 3D Conv layer\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(2, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            \n",
    "            # Second 3D Conv layer\n",
    "            nn.Conv3d(32, 16, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(16),\n",
    "            \n",
    "            # Final layer for change detection\n",
    "            nn.Conv3d(16, 1, kernel_size=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)  # temporal dimension 제거\n",
    "        return x\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    \"\"\"두 바운딩 박스 간의 IoU 계산\"\"\"\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    \n",
    "    intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    iou = intersection / float(area1 + area2 - intersection + 1e-6)\n",
    "    return iou\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25, iou_threshold=0.5):\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 3D CNN 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    temporal_model = Conv3DNet().to(device)\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # YOLO로 객체 검출\n",
    "    results1 = yolo_model(image1)[0]\n",
    "    results2 = yolo_model(image2)[0]\n",
    "    \n",
    "    # 3D CNN을 위한 입력 준비\n",
    "    imgs_tensor = torch.from_numpy(np.stack([image1, image2]))\n",
    "    imgs_tensor = imgs_tensor.permute(3, 0, 1, 2).unsqueeze(0).float() / 255.0\n",
    "    imgs_tensor = imgs_tensor.to(device)\n",
    "    \n",
    "    # 3D CNN으로 변화 감지\n",
    "    with torch.no_grad():\n",
    "        change_score = temporal_model(imgs_tensor)\n",
    "        change_score = change_score.squeeze()\n",
    "    \n",
    "    # 검출 결과 추출\n",
    "    boxes1 = []\n",
    "    boxes2 = []\n",
    "    \n",
    "    for r in results1.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes1.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    for r in results2.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, conf, cls = r\n",
    "        if conf > conf_threshold:\n",
    "            boxes2.append({\n",
    "                'box': [int(x1), int(y1), int(x2), int(y2)],\n",
    "                'conf': conf,\n",
    "                'class': int(cls),\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    # 객체 매칭 및 변화 감지\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    \n",
    "    for i, obj1 in enumerate(boxes1):\n",
    "        best_iou = iou_threshold\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, obj2 in enumerate(boxes2):\n",
    "            if obj2['matched'] or obj1['class'] != obj2['class']:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_iou(obj1['box'], obj2['box'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            boxes1[i]['matched'] = True\n",
    "            boxes2[best_match]['matched'] = True\n",
    "        else:\n",
    "            disappeared.append(obj1)\n",
    "    \n",
    "    # 새로 나타난 객체 찾기\n",
    "    for obj2 in boxes2:\n",
    "        if not obj2['matched']:\n",
    "            appeared.append(obj2)\n",
    "    \n",
    "    # 결과 시각화\n",
    "    def create_visualization(image1, image2, disappeared, appeared, change_score):\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 사라진 객체 표시 (빨간색)\n",
    "        for obj in disappeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img1, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "            cv2.putText(vis_img1, f'Disappeared ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체 표시 (초록색)\n",
    "        for obj in appeared:\n",
    "            x1, y1, x2, y2 = obj['box']\n",
    "            cv2.rectangle(vis_img2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(vis_img2, f'Appeared ({obj[\"conf\"]:.2f})', \n",
    "                       (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 변화 히트맵 생성\n",
    "        change_heatmap = change_score.cpu().numpy()\n",
    "        change_heatmap = (change_heatmap * 255).astype(np.uint8)\n",
    "        change_heatmap = cv2.applyColorMap(change_heatmap, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        \n",
    "        result = np.hstack((overlay1, overlay2))\n",
    "        return result\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image = create_visualization(image1, image2, disappeared, appeared, change_score)\n",
    "    \n",
    "    # 결과 출력\n",
    "    cv2.imshow('Object Changes (3D CNN)', result_image)\n",
    "    \n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Total objects in first image: {len(boxes1)}\")\n",
    "    print(f\"Total objects in second image: {len(boxes2)}\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 7.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 8.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    disappeared, appeared = detect_object_changes(\n",
    "        image1_path, \n",
    "        image2_path, \n",
    "        yolo_model_path,\n",
    "        conf_threshold=0.25,\n",
    "        iou_threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 crossarms, 1 polo, 9 wires, 19.3ms\n",
      "Speed: 2.0ms preprocess, 19.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 crossarms, 1 polo, 12 wires, 16.3ms\n",
      "Speed: 2.0ms preprocess, 16.3ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Total objects in first image: 4\n",
      "Total objects in second image: 4\n",
      "Disappeared objects: 0\n",
      "Appeared objects: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(2, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            \n",
    "            nn.Conv3d(32, 16, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(16),\n",
    "            \n",
    "            nn.Conv3d(16, 1, kernel_size=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "\n",
    "def calculate_mask_iou(mask1, mask2):\n",
    "    \"\"\"두 마스크 간의 IoU 계산\"\"\"\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    return intersection / (union + 1e-6)\n",
    "\n",
    "def calculate_distance(mask1, mask2):\n",
    "    \"\"\"두 마스크의 중심점 간 거리 계산\"\"\"\n",
    "    y1, x1 = np.where(mask1)\n",
    "    y2, x2 = np.where(mask2)\n",
    "    \n",
    "    center1 = np.array([np.mean(y1), np.mean(x1)])\n",
    "    center2 = np.array([np.mean(y2), np.mean(x2)])\n",
    "    \n",
    "    return np.linalg.norm(center1 - center2)\n",
    "\n",
    "def match_wires(masks1, masks2, distance_threshold=30):\n",
    "    \"\"\"전선 객체들을 매칭하는 함수\"\"\"\n",
    "    wire_masks1 = [m for m in masks1 if m['class'] == 2]  # wire class\n",
    "    wire_masks2 = [m for m in masks2 if m['class'] == 2]\n",
    "    \n",
    "    # 각 전선의 y좌표 평균값 계산\n",
    "    for w1 in wire_masks1:\n",
    "        w1['y_center'] = np.mean(np.where(w1['mask'])[0])\n",
    "    for w2 in wire_masks2:\n",
    "        w2['y_center'] = np.mean(np.where(w2['mask'])[0])\n",
    "    \n",
    "    # y좌표 기준으로 정렬\n",
    "    wire_masks1.sort(key=lambda x: x['y_center'])\n",
    "    wire_masks2.sort(key=lambda x: x['y_center'])\n",
    "    \n",
    "    # 순서대로 매칭\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    while i < len(wire_masks1) and j < len(wire_masks2):\n",
    "        w1 = wire_masks1[i]\n",
    "        w2 = wire_masks2[j]\n",
    "        \n",
    "        y_diff = abs(w1['y_center'] - w2['y_center'])\n",
    "        \n",
    "        if y_diff < 20:  # y좌표 차이가 20픽셀 이내\n",
    "            w1['matched'] = True\n",
    "            w2['matched'] = True\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif w1['y_center'] < w2['y_center']:\n",
    "            disappeared.append(w1)\n",
    "            i += 1\n",
    "        else:\n",
    "            appeared.append(w2)\n",
    "            j += 1\n",
    "    \n",
    "    # 남은 전선들 처리\n",
    "    while i < len(wire_masks1):\n",
    "        disappeared.append(wire_masks1[i])\n",
    "        i += 1\n",
    "    \n",
    "    while j < len(wire_masks2):\n",
    "        appeared.append(wire_masks2[j])\n",
    "        j += 1\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "def preprocess_masks(masks):\n",
    "    \"\"\"마스크 전처리 함수\"\"\"\n",
    "    processed = []\n",
    "    for mask in masks:\n",
    "        # 너무 작은 마스크 제거\n",
    "        if np.sum(mask['mask']) < 50:  # 50픽셀 미만 제거\n",
    "            continue\n",
    "        \n",
    "        # 전선 클래스의 경우 형태 검사\n",
    "        if mask['class'] == 2:  # wire class\n",
    "            h, w = mask['mask'].shape\n",
    "            if h/w < 2:  # 세로/가로 비율 2 미만 제외\n",
    "                continue\n",
    "            \n",
    "        processed.append(mask)\n",
    "    return processed\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25, iou_threshold=0.5):\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 3D CNN 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    temporal_model = Conv3DNet().to(device)\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # YOLO로 객체 검출 및 세그멘테이션\n",
    "    results1 = yolo_model(image1)[0]\n",
    "    results2 = yolo_model(image2)[0]\n",
    "    \n",
    "    # 3D CNN을 위한 입력 준비\n",
    "    imgs_tensor = torch.from_numpy(np.stack([image1, image2]))\n",
    "    imgs_tensor = imgs_tensor.permute(3, 0, 1, 2).unsqueeze(0).float() / 255.0\n",
    "    imgs_tensor = imgs_tensor.to(device)\n",
    "    \n",
    "    # 3D CNN으로 변화 감지\n",
    "    with torch.no_grad():\n",
    "        change_score = temporal_model(imgs_tensor)\n",
    "        change_score = change_score.squeeze()\n",
    "    \n",
    "    # 마스크 추출 및 전처리\n",
    "    masks1 = []\n",
    "    masks2 = []\n",
    "    \n",
    "    # 첫 번째 이미지의 마스크 추출\n",
    "    for i in range(len(results1.masks.data)):\n",
    "        conf = results1.boxes.conf[i].item()\n",
    "        if conf > conf_threshold:\n",
    "            mask = results1.masks.data[i].cpu().numpy()\n",
    "            cls = int(results1.boxes.cls[i].item())\n",
    "            masks1.append({\n",
    "                'mask': mask,\n",
    "                'conf': conf,\n",
    "                'class': cls,\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    # 두 번째 이미지의 마스크 추출\n",
    "    for i in range(len(results2.masks.data)):\n",
    "        conf = results2.boxes.conf[i].item()\n",
    "        if conf > conf_threshold:\n",
    "            mask = results2.masks.data[i].cpu().numpy()\n",
    "            cls = int(results2.boxes.cls[i].item())\n",
    "            masks2.append({\n",
    "                'mask': mask,\n",
    "                'conf': conf,\n",
    "                'class': cls,\n",
    "                'matched': False\n",
    "            })\n",
    "    \n",
    "    # 마스크 전처리\n",
    "    masks1 = preprocess_masks(masks1)\n",
    "    masks2 = preprocess_masks(masks2)\n",
    "    \n",
    "    # 객체 매칭 및 변화 감지\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    \n",
    "    # 전선 객체 먼저 처리\n",
    "    wire_disappeared, wire_appeared = match_wires(masks1, masks2)\n",
    "    disappeared.extend(wire_disappeared)\n",
    "    appeared.extend(wire_appeared)\n",
    "    \n",
    "    # 나머지 객체들 처리\n",
    "    for i, obj1 in enumerate(masks1):\n",
    "        if obj1['class'] == 2 or obj1['matched']:  # 전선이거나 이미 매칭된 객체는 건너뛰기\n",
    "            continue\n",
    "            \n",
    "        best_iou = iou_threshold\n",
    "        best_match = -1\n",
    "        \n",
    "        for j, obj2 in enumerate(masks2):\n",
    "            if obj2['class'] == 2 or obj2['matched'] or obj1['class'] != obj2['class']:\n",
    "                continue\n",
    "                \n",
    "            iou = calculate_mask_iou(obj1['mask'], obj2['mask'])\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match = j\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            masks1[i]['matched'] = True\n",
    "            masks2[best_match]['matched'] = True\n",
    "        else:\n",
    "            disappeared.append(obj1)\n",
    "    \n",
    "    # 남은 새로운 객체들 처리\n",
    "    for obj2 in masks2:\n",
    "        if obj2['class'] != 2 and not obj2['matched']:  # 전선이 아니고 매칭되지 않은 객체\n",
    "            appeared.append(obj2)\n",
    "\n",
    "    def create_visualization(image1, image2, disappeared, appeared, change_score):\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 사라진 객체의 마스크 표시 (빨간색)\n",
    "        for obj in disappeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img1[mask] = vis_img1[mask] * 0.7 + np.array([0, 0, 255]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img1, contours, -1, (0, 0, 255), 2)\n",
    "            \n",
    "            # 신뢰도 표시\n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                cv2.putText(vis_img1, f'Disappeared ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체의 마스크 표시 (초록색)\n",
    "        for obj in appeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img2[mask] = vis_img2[mask] * 0.7 + np.array([0, 255, 0]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img2, contours, -1, (0, 255, 0), 2)\n",
    "            \n",
    "            # 신뢰도 표시\n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                cv2.putText(vis_img2, f'Appeared ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 변화 히트맵 생성\n",
    "        change_heatmap = change_score.cpu().numpy()\n",
    "        change_heatmap = (change_heatmap * 255).astype(np.uint8)\n",
    "        change_heatmap = cv2.applyColorMap(change_heatmap, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        \n",
    "        result = np.hstack((overlay1, overlay2))\n",
    "        return result\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image = create_visualization(image1, image2, disappeared, appeared, change_score)\n",
    "    \n",
    "    # 결과 출력\n",
    "    cv2.imshow('Object Changes (Segmentation)', result_image)\n",
    "    \n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Total objects in first image: {len(masks1)}\")\n",
    "    print(f\"Total objects in second image: {len(masks2)}\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            print(f\"Class: {obj['class']}, Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 1.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 2.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    disappeared, appeared = detect_object_changes(\n",
    "        image1_path, \n",
    "        image2_path, \n",
    "        yolo_model_path,\n",
    "        conf_threshold=0.25,\n",
    "        iou_threshold=0.5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 crossarm, 1 polo, 13 wires, 18.9ms\n",
      "Speed: 2.0ms preprocess, 18.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 crossarm, 1 polo, 15 wires, 19.9ms\n",
      "Speed: 2.0ms preprocess, 19.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Disappeared objects: 0\n",
      "Appeared objects: 2\n",
      "\n",
      "Appeared objects details:\n",
      "wire: Confidence: 0.94\n",
      "wire: Confidence: 0.93\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(2, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            \n",
    "            nn.Conv3d(32, 16, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(16),\n",
    "            \n",
    "            nn.Conv3d(16, 1, kernel_size=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "\n",
    "def analyze_image_difference(image1, image2):\n",
    "    \"\"\"두 이미지 간의 실제 차이 분석\"\"\"\n",
    "    # 그레이스케일 변환\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 이미지 차분\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    \n",
    "    # 노이즈 제거\n",
    "    diff = cv2.GaussianBlur(diff, (5, 5), 0)\n",
    "    \n",
    "    # 임계값 처리\n",
    "    _, thresh = cv2.threshold(diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return thresh\n",
    "\n",
    "def match_masks(mask1, mask2, diff_mask):\n",
    "    \"\"\"두 마스크 간의 매칭 여부 판단\"\"\"\n",
    "    # 마스크 영역에서의 차이 계산\n",
    "    overlap = np.logical_and(mask1, mask2)\n",
    "    diff_region = np.logical_and(diff_mask, np.logical_or(mask1, mask2))\n",
    "    \n",
    "    # 변화율 계산\n",
    "    total_area = np.sum(mask1) + np.sum(mask2)\n",
    "    if total_area == 0:\n",
    "        return False\n",
    "        \n",
    "    change_ratio = np.sum(diff_region) / total_area * 2\n",
    "    \n",
    "    # 오버랩 비율 계산\n",
    "    min_area = min(np.sum(mask1), np.sum(mask2))\n",
    "    if min_area == 0:\n",
    "        return False\n",
    "        \n",
    "    overlap_ratio = np.sum(overlap) / min_area\n",
    "    \n",
    "    # 매칭 판단\n",
    "    return change_ratio < 0.3 and overlap_ratio > 0.5\n",
    "\n",
    "def detect_changes(results1, results2, diff_mask, conf_threshold=0.25):\n",
    "    \"\"\"변화 감지\"\"\"\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    \n",
    "    # 첫 번째 이미지의 객체들 처리\n",
    "    matched1 = set()\n",
    "    for i in range(len(results1.boxes)):\n",
    "        if results1.boxes.conf[i].item() < conf_threshold:\n",
    "            continue\n",
    "            \n",
    "        cls1 = int(results1.boxes.cls[i].item())\n",
    "        mask1 = results1.masks.data[i].cpu().numpy()\n",
    "        matched = False\n",
    "        \n",
    "        # 두 번째 이미지의 같은 클래스 객체들과 비교\n",
    "        for j in range(len(results2.boxes)):\n",
    "            if results2.boxes.conf[j].item() < conf_threshold:\n",
    "                continue\n",
    "                \n",
    "            cls2 = int(results2.boxes.cls[j].item())\n",
    "            if cls1 != cls2:\n",
    "                continue\n",
    "                \n",
    "            mask2 = results2.masks.data[j].cpu().numpy()\n",
    "            if match_masks(mask1, mask2, diff_mask):\n",
    "                matched = True\n",
    "                matched1.add(i)\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            disappeared.append({\n",
    "                'mask': mask1,\n",
    "                'conf': results1.boxes.conf[i].item(),\n",
    "                'class': cls1\n",
    "            })\n",
    "    \n",
    "    # 두 번째 이미지의 새로운 객체들 처리\n",
    "    for j in range(len(results2.boxes)):\n",
    "        if results2.boxes.conf[j].item() < conf_threshold:\n",
    "            continue\n",
    "            \n",
    "        cls2 = int(results2.boxes.cls[j].item())\n",
    "        mask2 = results2.masks.data[j].cpu().numpy()\n",
    "        matched = False\n",
    "        \n",
    "        # 첫 번째 이미지의 같은 클래스 객체들과 비교\n",
    "        for i in range(len(results1.boxes)):\n",
    "            if results1.boxes.conf[i].item() < conf_threshold:\n",
    "                continue\n",
    "                \n",
    "            cls1 = int(results1.boxes.cls[i].item())\n",
    "            if cls1 != cls2:\n",
    "                continue\n",
    "                \n",
    "            mask1 = results1.masks.data[i].cpu().numpy()\n",
    "            if match_masks(mask1, mask2, diff_mask):\n",
    "                matched = True\n",
    "                break\n",
    "        \n",
    "        if not matched:\n",
    "            appeared.append({\n",
    "                'mask': mask2,\n",
    "                'conf': results2.boxes.conf[j].item(),\n",
    "                'class': cls2\n",
    "            })\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25):\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 3D CNN 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    temporal_model = Conv3DNet().to(device)\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # 이미지 차분 분석\n",
    "    diff_mask = analyze_image_difference(image1, image2)\n",
    "    \n",
    "    # YOLO로 객체 검출 및 세그멘테이션\n",
    "    results1 = yolo_model(image1)[0]\n",
    "    results2 = yolo_model(image2)[0]\n",
    "    \n",
    "    # 변화 감지\n",
    "    disappeared, appeared = detect_changes(results1, results2, diff_mask, conf_threshold)\n",
    "    \n",
    "    # 3D CNN을 위한 입력 준비\n",
    "    imgs_tensor = torch.from_numpy(np.stack([image1, image2]))\n",
    "    imgs_tensor = imgs_tensor.permute(3, 0, 1, 2).unsqueeze(0).float() / 255.0\n",
    "    imgs_tensor = imgs_tensor.to(device)\n",
    "    \n",
    "    # 3D CNN으로 변화 감지\n",
    "    with torch.no_grad():\n",
    "        change_score = temporal_model(imgs_tensor)\n",
    "        change_score = change_score.squeeze()\n",
    "    \n",
    "    # 클래스 이름 매핑\n",
    "    class_names = {\n",
    "        0: 'crossarm',\n",
    "        1: 'pole',\n",
    "        2: 'wire'\n",
    "    }\n",
    "    \n",
    "    # 결과 시각화\n",
    "    def create_visualization(image1, image2, disappeared, appeared, change_score, diff_mask):\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 차분 결과 시각화 (선택적)\n",
    "        diff_vis = cv2.cvtColor(diff_mask, cv2.COLOR_GRAY2BGR)\n",
    "        \n",
    "        # 사라진 객체의 마스크 표시 (빨간색)\n",
    "        for obj in disappeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img1[mask] = vis_img1[mask] * 0.7 + np.array([0, 0, 255]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img1, contours, -1, (0, 0, 255), 2)\n",
    "            \n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "                cv2.putText(vis_img1, f'Disappeared {class_name} ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체의 마스크 표시 (초록색)\n",
    "        for obj in appeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img2[mask] = vis_img2[mask] * 0.7 + np.array([0, 255, 0]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img2, contours, -1, (0, 255, 0), 2)\n",
    "            \n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "                cv2.putText(vis_img2, f'Appeared {class_name} ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 변화 히트맵 생성\n",
    "        change_heatmap = change_score.cpu().numpy()\n",
    "        change_heatmap = (change_heatmap * 255).astype(np.uint8)\n",
    "        change_heatmap = cv2.applyColorMap(change_heatmap, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        \n",
    "        result = np.hstack((overlay1, overlay2))\n",
    "        return result\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image = create_visualization(image1, image2, disappeared, appeared, \n",
    "                                      change_score, diff_mask)\n",
    "    \n",
    "    # 결과 출력\n",
    "    cv2.imshow('Object Changes (Difference-based)', result_image)\n",
    "    \n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "            print(f\"{class_name}: Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "            print(f\"{class_name}: Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 3.jpg'\n",
    "    image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 4.jpg'\n",
    "    yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "    \n",
    "    disappeared, appeared = detect_object_changes(\n",
    "        image1_path, \n",
    "        image2_path, \n",
    "        yolo_model_path,\n",
    "        conf_threshold=0.25\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 4 crossarms, 1 polo, 15 wires, 19.9ms\n",
      "Speed: 2.0ms preprocess, 19.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 crossarms, 1 polo, 19 wires, 18.9ms\n",
      "Speed: 2.0ms preprocess, 18.9ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "Change Detection Results:\n",
      "Disappeared objects: 1\n",
      "Appeared objects: 5\n",
      "\n",
      "Disappeared objects details:\n",
      "wire: Confidence: 0.80\n",
      "\n",
      "Appeared objects details:\n",
      "wire: Confidence: 0.95\n",
      "wire: Confidence: 0.94\n",
      "wire: Confidence: 0.91\n",
      "wire: Confidence: 0.91\n",
      "wire: Confidence: 0.86\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "\n",
    "class Conv3DNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Conv3DNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, 32, kernel_size=(2, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(32),\n",
    "            \n",
    "            nn.Conv3d(32, 16, kernel_size=(1, 3, 3), padding=(0, 1, 1)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm3d(16),\n",
    "            \n",
    "            nn.Conv3d(16, 1, kernel_size=(1, 1, 1)),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.squeeze(2)\n",
    "        return x\n",
    "\n",
    "def verify_wire_match(mask1, mask2, group_info=None):\n",
    "    \"\"\"개선된 wire 매칭 검증\"\"\"\n",
    "    y1, x1 = np.where(mask1)\n",
    "    y2, x2 = np.where(mask2)\n",
    "    \n",
    "    if len(x1) < 2 or len(x2) < 2:\n",
    "        return False, 0.0\n",
    "    \n",
    "    try:\n",
    "        # 기울기 계산\n",
    "        coef1 = np.polyfit(x1, y1, 1)\n",
    "        coef2 = np.polyfit(x2, y2, 1)\n",
    "        slope1, slope2 = coef1[0], coef2[0]\n",
    "        \n",
    "        # 중심점 계산\n",
    "        center1 = (np.mean(x1), np.mean(y1))\n",
    "        center2 = (np.mean(x2), np.mean(y2))\n",
    "        \n",
    "        # 수직 거리 계산\n",
    "        vertical_dist = abs(center1[1] - center2[1])\n",
    "        \n",
    "        # 기울기 차이\n",
    "        slope_diff = abs(slope1 - slope2)\n",
    "        \n",
    "        # 평행선 그룹 검사\n",
    "        if group_info is not None:\n",
    "            group1 = group_info.get(center1[1], [])\n",
    "            group2 = group_info.get(center2[1], [])\n",
    "            if group1 and group2:\n",
    "                avg_spacing1 = np.mean(np.diff(sorted([y for y, _ in group1])))\n",
    "                avg_spacing2 = np.mean(np.diff(sorted([y for y, _ in group2])))\n",
    "                spacing_match = abs(avg_spacing1 - avg_spacing2) < 10\n",
    "            else:\n",
    "                spacing_match = True\n",
    "        else:\n",
    "            spacing_match = True\n",
    "        \n",
    "        # 매칭 조건 강화\n",
    "        matched = (\n",
    "            vertical_dist < 15 and    # 수직 거리 15픽셀 이내\n",
    "            slope_diff < 0.1 and      # 기울기 차이 0.1 이내\n",
    "            spacing_match             # 평행선 간격 유사\n",
    "        )\n",
    "        \n",
    "        score = 1.0 - (vertical_dist / 30 + slope_diff)\n",
    "        return matched, max(0, min(1, score))\n",
    "        \n",
    "    except:\n",
    "        return False, 0.0\n",
    "\n",
    "def verify_crossarm_match(mask1, mask2):\n",
    "    \"\"\"crossarm 매칭을 위한 특별 검증\"\"\"\n",
    "    y1, x1 = np.where(mask1)\n",
    "    y2, x2 = np.where(mask2)\n",
    "    \n",
    "    if len(x1) < 2 or len(x2) < 2:\n",
    "        return False, 0.0\n",
    "    \n",
    "    # 중심점 계산\n",
    "    center1 = (np.mean(x1), np.mean(y1))\n",
    "    center2 = (np.mean(x2), np.mean(y2))\n",
    "    \n",
    "    # 거리 계산\n",
    "    center_dist = np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "    \n",
    "    # crossarm의 경우 위치가 거의 동일해야 함\n",
    "    return center_dist < 15, 1.0 - (center_dist / 15)\n",
    "\n",
    "def analyze_image_difference(image1, image2):\n",
    "    \"\"\"이미지 차분 분석\"\"\"\n",
    "    # 이미지 전처리\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # CLAHE 적용\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    gray1 = clahe.apply(gray1)\n",
    "    gray2 = clahe.apply(gray2)\n",
    "    \n",
    "    # 노이즈 제거\n",
    "    gray1 = cv2.GaussianBlur(gray1, (5, 5), 0)\n",
    "    gray2 = cv2.GaussianBlur(gray2, (5, 5), 0)\n",
    "    \n",
    "    # 이미지 차분\n",
    "    diff = cv2.absdiff(gray1, gray2)\n",
    "    \n",
    "    # 적응형 임계값 처리\n",
    "    diff = cv2.adaptiveThreshold(\n",
    "        diff,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        11,\n",
    "        2\n",
    "    )\n",
    "    \n",
    "    # 노이즈 제거를 위한 모폴로지 연산\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    diff = cv2.morphologyEx(diff, cv2.MORPH_OPEN, kernel)\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def match_masks(mask1, mask2, diff_mask):\n",
    "    \"\"\"일반적인 마스크 매칭\"\"\"\n",
    "    overlap = np.logical_and(mask1, mask2)\n",
    "    diff_region = np.logical_and(diff_mask, np.logical_or(mask1, mask2))\n",
    "    \n",
    "    total_area = np.sum(mask1) + np.sum(mask2)\n",
    "    if total_area == 0:\n",
    "        return False, 0.0\n",
    "    \n",
    "    min_area = min(np.sum(mask1), np.sum(mask2))\n",
    "    overlap_ratio = np.sum(overlap) / min_area\n",
    "    change_ratio = np.sum(diff_region) / total_area * 2\n",
    "    \n",
    "    # 완화된 매칭 조건\n",
    "    matched = (change_ratio < 0.3 and overlap_ratio > 0.6)\n",
    "    score = overlap_ratio * (1 - change_ratio)\n",
    "    \n",
    "    return matched, score\n",
    "\n",
    "def detect_changes(results1, results2, diff_mask, conf_threshold=0.25):\n",
    "    \"\"\"변화 감지 함수 수정\"\"\"\n",
    "    disappeared = []\n",
    "    appeared = []\n",
    "    matched_pairs = set()\n",
    "    \n",
    "    # wire 객체만 추출\n",
    "    wires1 = [(i, results1.masks.data[i]) for i in range(len(results1.boxes)) \n",
    "             if int(results1.boxes.cls[i].item()) == 2 \n",
    "             and results1.boxes.conf[i].item() > conf_threshold]\n",
    "    \n",
    "    wires2 = [(i, results2.masks.data[i]) for i in range(len(results2.boxes)) \n",
    "             if int(results2.boxes.cls[i].item()) == 2 \n",
    "             and results2.boxes.conf[i].item() > conf_threshold]\n",
    "    \n",
    "    # wire 그룹화\n",
    "    def group_wires(wires):\n",
    "        groups = {}\n",
    "        for idx, mask in wires:\n",
    "            y_center = np.mean(np.where(mask.cpu().numpy())[0])\n",
    "            groups.setdefault(int(y_center // 20) * 20, []).append((y_center, idx))\n",
    "        return groups\n",
    "    \n",
    "    wire_groups1 = group_wires(wires1)\n",
    "    wire_groups2 = group_wires(wires2)\n",
    "    \n",
    "    # wire 매칭\n",
    "    for idx1, mask1 in wires1:\n",
    "        if idx1 in [p[0] for p in matched_pairs]:\n",
    "            continue\n",
    "            \n",
    "        best_match = -1\n",
    "        best_score = -1\n",
    "        \n",
    "        for idx2, mask2 in wires2:\n",
    "            if idx2 in [p[1] for p in matched_pairs]:\n",
    "                continue\n",
    "                \n",
    "            matched, score = verify_wire_match(\n",
    "                mask1.cpu().numpy(), \n",
    "                mask2.cpu().numpy(),\n",
    "                group_info=wire_groups2\n",
    "            )\n",
    "            \n",
    "            if matched and score > best_score:\n",
    "                best_score = score\n",
    "                best_match = idx2\n",
    "        \n",
    "        if best_match >= 0:\n",
    "            matched_pairs.add((idx1, best_match))\n",
    "        else:\n",
    "            disappeared.append({\n",
    "                'mask': mask1.cpu().numpy(),\n",
    "                'conf': results1.boxes.conf[idx1].item(),\n",
    "                'class': 2\n",
    "            })\n",
    "    \n",
    "    # 매칭되지 않은 wire 처리\n",
    "    for idx2, mask2 in wires2:\n",
    "        if idx2 not in [p[1] for p in matched_pairs]:\n",
    "            appeared.append({\n",
    "                'mask': mask2.cpu().numpy(),\n",
    "                'conf': results2.boxes.conf[idx2].item(),\n",
    "                'class': 2\n",
    "            })\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "def detect_object_changes(image1_path, image2_path, yolo_model_path, conf_threshold=0.25):\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    # YOLO 모델 로드\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # 3D CNN 모델 초기화\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    temporal_model = Conv3DNet().to(device)\n",
    "    \n",
    "    # 이미지 로드 및 전처리\n",
    "    image1 = cv2.imread(image1_path)\n",
    "    image2 = cv2.imread(image2_path)\n",
    "    \n",
    "    # 이미지 크기 조정\n",
    "    target_size = (640, 640)\n",
    "    image1 = cv2.resize(image1, target_size)\n",
    "    image2 = cv2.resize(image2, target_size)\n",
    "    \n",
    "    # 이미지 차분 분석\n",
    "    diff_mask = analyze_image_difference(image1, image2)\n",
    "    \n",
    "    # YOLO로 객체 검출\n",
    "    results1 = yolo_model(image1)[0]\n",
    "    results2 = yolo_model(image2)[0]\n",
    "    \n",
    "    # 변화 감지\n",
    "    disappeared, appeared = detect_changes(results1, results2, diff_mask, conf_threshold)\n",
    "    \n",
    "    # 3D CNN 처리\n",
    "    imgs_tensor = torch.from_numpy(np.stack([image1, image2]))\n",
    "    imgs_tensor = imgs_tensor.permute(3, 0, 1, 2).unsqueeze(0).float() / 255.0\n",
    "    imgs_tensor = imgs_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        change_score = temporal_model(imgs_tensor)\n",
    "        change_score = change_score.squeeze()\n",
    "    \n",
    "    # 시각화 및 결과 출력\n",
    "    class_names = {0: 'crossarm', 1: 'pole', 2: 'wire'}\n",
    "    \n",
    "    def create_visualization(image1, image2, disappeared, appeared, change_score):\n",
    "        vis_img1 = image1.copy()\n",
    "        vis_img2 = image2.copy()\n",
    "        \n",
    "        # 사라진 객체 표시\n",
    "        for obj in disappeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img1[mask] = vis_img1[mask] * 0.7 + np.array([0, 0, 255]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img1, contours, -1, (0, 0, 255), 2)\n",
    "            \n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "                cv2.putText(vis_img1, f'Disappeared {class_name} ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
    "        \n",
    "        # 새로 나타난 객체 표시\n",
    "        for obj in appeared:\n",
    "            mask = obj['mask'].astype(bool)\n",
    "            vis_img2[mask] = vis_img2[mask] * 0.7 + np.array([0, 255, 0]) * 0.3\n",
    "            \n",
    "            contours, _ = cv2.findContours((mask * 255).astype(np.uint8), \n",
    "                                         cv2.RETR_EXTERNAL, \n",
    "                                         cv2.CHAIN_APPROX_SIMPLE)\n",
    "            cv2.drawContours(vis_img2, contours, -1, (0, 255, 0), 2)\n",
    "            \n",
    "            M = cv2.moments(mask.astype(np.uint8))\n",
    "            if M[\"m00\"] != 0:\n",
    "                cx = int(M[\"m10\"] / M[\"m00\"])\n",
    "                cy = int(M[\"m01\"] / M[\"m00\"])\n",
    "                class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "                cv2.putText(vis_img2, f'Appeared {class_name} ({obj[\"conf\"]:.2f})', \n",
    "                           (cx-60, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # 히트맵 처리\n",
    "        change_heatmap = change_score.cpu().numpy()\n",
    "        change_heatmap = (change_heatmap * 255).astype(np.uint8)\n",
    "        change_heatmap = cv2.applyColorMap(change_heatmap, cv2.COLORMAP_JET)\n",
    "        \n",
    "        # 결과 합치기\n",
    "        alpha = 0.3\n",
    "        overlay1 = cv2.addWeighted(vis_img1, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        overlay2 = cv2.addWeighted(vis_img2, 1 - alpha, change_heatmap, alpha, 0)\n",
    "        \n",
    "        return np.hstack((overlay1, overlay2))\n",
    "    \n",
    "    # 시각화 실행\n",
    "    result_image = create_visualization(image1, image2, disappeared, appeared, change_score)\n",
    "    cv2.imshow('Object Changes', result_image)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(\"\\nChange Detection Results:\")\n",
    "    print(f\"Disappeared objects: {len(disappeared)}\")\n",
    "    print(f\"Appeared objects: {len(appeared)}\")\n",
    "    \n",
    "    if disappeared:\n",
    "        print(\"\\nDisappeared objects details:\")\n",
    "        for obj in disappeared:\n",
    "            class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "            print(f\"{class_name}: Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    if appeared:\n",
    "        print(\"\\nAppeared objects details:\")\n",
    "        for obj in appeared:\n",
    "            class_name = class_names.get(obj['class'], f'class_{obj[\"class\"]}')\n",
    "            print(f\"{class_name}: Confidence: {obj['conf']:.2f}\")\n",
    "    \n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return disappeared, appeared\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   image1_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 5.jpg'\n",
    "   image2_path = 'C:/Users/AI-LHJ/Downloads/source/ch5/test 6.jpg'\n",
    "   yolo_model_path = 'C:/Users/AI-LHJ/Desktop/YOLOv11/runs/segment/train11/weights/best.pt'\n",
    "   \n",
    "   disappeared, appeared = detect_object_changes(\n",
    "       image1_path, \n",
    "       image2_path, \n",
    "       yolo_model_path,\n",
    "       conf_threshold=0.25\n",
    "   )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
